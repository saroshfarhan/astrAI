{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e13e0670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: JUPITER | points: 1024 | device: cpu\n",
      "MLP | epoch 01 | val_loss=0.2263 | microF1=0.910\n",
      "MLP | epoch 02 | val_loss=0.1707 | microF1=0.932\n",
      "MLP | epoch 03 | val_loss=0.1377 | microF1=0.946\n",
      "MLP | epoch 04 | val_loss=0.1320 | microF1=0.944\n",
      "MLP | epoch 05 | val_loss=0.1185 | microF1=0.951\n",
      "MLP | epoch 06 | val_loss=0.1196 | microF1=0.951\n",
      "MLP | epoch 07 | val_loss=0.1142 | microF1=0.953\n",
      "CNN1D | epoch 01 | val_loss=0.6899 | microF1=0.644\n",
      "CNN1D | epoch 02 | val_loss=0.6768 | microF1=0.570\n",
      "CNN1D | epoch 03 | val_loss=0.6501 | microF1=0.579\n",
      "CNN1D | epoch 04 | val_loss=0.6132 | microF1=0.654\n",
      "CNN1D | epoch 05 | val_loss=0.5976 | microF1=0.648\n",
      "CNN1D | epoch 06 | val_loss=0.5947 | microF1=0.685\n",
      "CNN1D | epoch 07 | val_loss=0.5853 | microF1=0.660\n",
      "GRU | epoch 01 | val_loss=0.6920 | microF1=0.604\n",
      "GRU | epoch 02 | val_loss=0.6886 | microF1=0.568\n",
      "GRU | epoch 03 | val_loss=0.6819 | microF1=0.622\n",
      "GRU | epoch 04 | val_loss=0.6609 | microF1=0.644\n",
      "GRU | epoch 05 | val_loss=0.6591 | microF1=0.646\n",
      "GRU | epoch 06 | val_loss=0.6555 | microF1=0.655\n",
      "GRU | epoch 07 | val_loss=0.6518 | microF1=0.647\n",
      "\n",
      "=== Model comparison (lower loss better) ===\n",
      "MLP   | val_loss=0.1142 | microF1=0.953 | train_time=0.5s\n",
      "CNN1D | val_loss=0.5853 | microF1=0.660 | train_time=7.8s\n",
      "GRU   | val_loss=0.6518 | microF1=0.647 | train_time=56.7s\n",
      "\n",
      "Best model: MLP | val_loss=0.1142 | microF1=0.953\n",
      "\n",
      "Predicted probabilities on REAL Jupiter spectrum:\n",
      " NH3: 0.999\n",
      "C2H6: 0.579\n",
      "C2H2: 0.459\n",
      " CH4: 0.175\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Imports + config\n",
    "# =========================\n",
    "import pickle, numpy as np, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "PKL_PATH = \"JUPITER_MASTER_SPECTRA.pkl\"\n",
    "\n",
    "# Speed knobs\n",
    "SEED = 7\n",
    "N_RESAMPLE = 1024        # 512 if you want even faster\n",
    "N_SYNTH = 1500           # 800–2000 is plenty for dry run\n",
    "EPOCHS = 7               # 5–10\n",
    "BATCH = 128\n",
    "LR = 1e-3\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Label space: Jupiter UV -> atmospheric species (not \"elements/minerals\")\n",
    "SPECIES = [\"CH4\", \"NH3\", \"C2H2\", \"C2H6\"]\n",
    "\n",
    "# Simple band templates (refine later using real line/band libraries)\n",
    "BANDS = {\n",
    "    \"CH4\": [(2350, 220), (2750, 260)],\n",
    "    \"NH3\": [(2050,  90), (2150,  90)],\n",
    "    \"C2H2\": [(2700, 110), (2810, 100)],\n",
    "    \"C2H6\": [(2400, 140), (2550, 120)],\n",
    "}\n",
    "\n",
    "def gaussian(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "# =========================\n",
    "# 1) Load + clean real spectrum\n",
    "# =========================\n",
    "with open(PKL_PATH, \"rb\") as f:\n",
    "    real = pickle.load(f)\n",
    "\n",
    "w = np.asarray(real[\"wavelength\"], dtype=float)\n",
    "f = np.asarray(real[\"flux\"], dtype=float)\n",
    "\n",
    "mask = np.isfinite(w) & np.isfinite(f)\n",
    "w, f = w[mask], f[mask]\n",
    "idx = np.argsort(w)\n",
    "w, f = w[idx], f[idx]\n",
    "\n",
    "print(\"Loaded:\", real.get(\"target\", \"unknown\"), \"| points:\", len(w), \"| device:\", device)\n",
    "\n",
    "# =========================\n",
    "# 2) Resample to fixed length (fast + consistent)\n",
    "# =========================\n",
    "def resample_to_fixed(wave, flux, n=N_RESAMPLE):\n",
    "    w_new = np.linspace(wave.min(), wave.max(), n)\n",
    "    f_new = np.interp(w_new, wave, flux)\n",
    "    return w_new.astype(np.float32), f_new.astype(np.float32)\n",
    "\n",
    "w_fix, f_fix = resample_to_fixed(w, f, N_RESAMPLE)\n",
    "\n",
    "# =========================\n",
    "# 3) Preprocess -> channels (C, N)\n",
    "# =========================\n",
    "def robust_norm(x):\n",
    "    med = np.median(x)\n",
    "    iqr = np.percentile(x, 75) - np.percentile(x, 25)\n",
    "    if iqr <= 0:\n",
    "        iqr = 1.0\n",
    "    return (x - med) / iqr\n",
    "\n",
    "def make_channels(wave, flux):\n",
    "    x = robust_norm(flux)\n",
    "    d1 = np.gradient(x, wave)\n",
    "    d2 = np.gradient(d1, wave)\n",
    "    X = np.stack([x, d1, d2], axis=0).astype(np.float32)  # (3, N)\n",
    "    return X\n",
    "\n",
    "X_real = make_channels(w_fix, f_fix)   # (3, N)\n",
    "X_real_t = torch.tensor(X_real[None, ...], dtype=torch.float32).to(device)  # (1,3,N)\n",
    "\n",
    "# =========================\n",
    "# 4) Synthetic data generator\n",
    "# =========================\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "\n",
    "# 1) Build a smooth baseline from the REAL planet spectrum (full range)\n",
    "# window_length must be odd and < len(w_fix)\n",
    "WL = 151 if len(w_fix) > 151 else (len(w_fix)//2)*2 - 1\n",
    "baseline = savgol_filter(f_fix.astype(float), window_length=WL, polyorder=3)\n",
    "\n",
    "# Prevent weird negatives if any\n",
    "baseline = np.clip(baseline, np.percentile(baseline, 1), np.percentile(baseline, 99))\n",
    "\n",
    "def synth_spectrum(wave, labels):\n",
    "    # 2) Use anchored baseline instead of invented polynomial\n",
    "    # Add small continuum variation so model doesn't memorize exact baseline\n",
    "    cont = baseline.copy()\n",
    "\n",
    "    # small multiplicative drift (keeps shape, varies scale)\n",
    "    drift = 1.0 + rng.normal(0, 0.01) + rng.normal(0, 0.005) * ((wave - wave.min()) / (wave.max() - wave.min()) - 0.5)\n",
    "    cont = cont * drift\n",
    "\n",
    "    spec = cont.copy()\n",
    "\n",
    "    # 3) Absorption dips (same as before, with center jitter)\n",
    "    for sp, present in labels.items():\n",
    "        if not present:\n",
    "            continue\n",
    "        for (c, w0) in BANDS[sp]:\n",
    "            if sp == \"CH4\":\n",
    "                depth = rng.uniform(0.08, 0.25)\n",
    "                width = w0 * rng.uniform(0.9, 1.6)\n",
    "            else:\n",
    "                depth = rng.uniform(0.03, 0.15)\n",
    "                width = w0 * rng.uniform(0.7, 1.3)\n",
    "\n",
    "            c_jit = c + rng.normal(0, 10)\n",
    "            width = width * rng.uniform(0.9, 1.1)\n",
    "\n",
    "            dip = 1.0 - depth * gaussian(wave, c_jit, width)\n",
    "            spec *= dip\n",
    "\n",
    "    # 4) Noise: proportional to signal level (more realistic)\n",
    "    sigma = 0.01 * (np.max(spec) - np.min(spec) + 1e-8)\n",
    "    noise = rng.normal(0, sigma, size=wave.shape[0])\n",
    "    noise = np.convolve(noise, np.ones(7)/7, mode=\"same\")\n",
    "    spec = spec + noise\n",
    "\n",
    "    return spec.astype(np.float32)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Models: MLP, CNN, GRU\n",
    "# =========================\n",
    "K = len(SPECIES)\n",
    "C = 3\n",
    "N = N_RESAMPLE\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, c=C, n=N, k=K):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c*n, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, k)\n",
    "        )\n",
    "    def forward(self, x):  # x: (B,C,N)\n",
    "        return self.net(x)\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, in_ch=C, k=K):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 32, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, k)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, c=C, k=K, hidden=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=c, hidden_size=hidden, num_layers=1,\n",
    "                          batch_first=True, bidirectional=False)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, k)\n",
    "        )\n",
    "    def forward(self, x):  # x: (B,C,N) -> (B,N,C)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, h = self.gru(x)          # h: (1,B,H)\n",
    "        h = h[-1]                   # (B,H)\n",
    "        return self.head(h)\n",
    "\n",
    "# =========================\n",
    "# 6) Train + evaluate utilities\n",
    "# =========================\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def micro_f1_from_logits(logits, y_true, thr=0.5):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    y_hat = (probs >= thr).float()\n",
    "    tp = (y_hat * y_true).sum()\n",
    "    fp = (y_hat * (1 - y_true)).sum()\n",
    "    fn = ((1 - y_hat) * y_true).sum()\n",
    "    denom = (2*tp + fp + fn).clamp(min=1e-8)\n",
    "    return (2*tp / denom).item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_logits, all_y = [], []\n",
    "    for xb, yb in val_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        total_loss += loss_fn(logits, yb).item() * xb.size(0)\n",
    "        all_logits.append(logits)\n",
    "        all_y.append(yb)\n",
    "    total_loss /= len(X_val)\n",
    "    logits = torch.cat(all_logits, dim=0)\n",
    "    y_true = torch.cat(all_y, dim=0)\n",
    "    f1 = micro_f1_from_logits(logits, y_true)\n",
    "    return total_loss, f1\n",
    "\n",
    "def train_model(model, name):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    best = {\"loss\": 1e9, \"state\": None, \"f1\": 0.0}\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        vloss, vf1 = evaluate(model)\n",
    "        if vloss < best[\"loss\"]:\n",
    "            best = {\"loss\": vloss, \"state\": {k: v.detach().cpu() for k, v in model.state_dict().items()}, \"f1\": vf1}\n",
    "        print(f\"{name} | epoch {epoch:02d} | val_loss={vloss:.4f} | microF1={vf1:.3f}\")\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    return best, dt\n",
    "\n",
    "# =========================\n",
    "# 7) Train all three\n",
    "# =========================\n",
    "results = []\n",
    "\n",
    "mlp = MLP()\n",
    "best_mlp, t_mlp = train_model(mlp, \"MLP\")\n",
    "results.append((\"MLP\", best_mlp[\"loss\"], best_mlp[\"f1\"], t_mlp, best_mlp))\n",
    "\n",
    "cnn = CNN1D()\n",
    "best_cnn, t_cnn = train_model(cnn, \"CNN1D\")\n",
    "results.append((\"CNN1D\", best_cnn[\"loss\"], best_cnn[\"f1\"], t_cnn, best_cnn))\n",
    "\n",
    "gru = GRUClassifier(hidden=64)\n",
    "best_gru, t_gru = train_model(gru, \"GRU\")\n",
    "results.append((\"GRU\", best_gru[\"loss\"], best_gru[\"f1\"], t_gru, best_gru))\n",
    "\n",
    "print(\"\\n=== Model comparison (lower loss better) ===\")\n",
    "for name, lossv, f1v, dt, _ in sorted(results, key=lambda x: x[1]):\n",
    "    print(f\"{name:5s} | val_loss={lossv:.4f} | microF1={f1v:.3f} | train_time={dt:.1f}s\")\n",
    "\n",
    "# pick best by val_loss\n",
    "best_name, best_loss, best_f1, best_dt, best_blob = sorted(results, key=lambda x: x[1])[0]\n",
    "print(f\"\\nBest model: {best_name} | val_loss={best_loss:.4f} | microF1={best_f1:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# 8) Inference on real Jupiter spectrum\n",
    "# =========================\n",
    "def load_best_model(name, state):\n",
    "    if name == \"MLP\":\n",
    "        m = MLP()\n",
    "    elif name == \"CNN1D\":\n",
    "        m = CNN1D()\n",
    "    else:\n",
    "        m = GRUClassifier(hidden=64)\n",
    "    m.load_state_dict(state)\n",
    "    m.to(device).eval()\n",
    "    return m\n",
    "\n",
    "best_model = load_best_model(best_name, best_blob[\"state\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = best_model(X_real_t)[0]\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "print(\"\\nPredicted probabilities on REAL Jupiter spectrum:\")\n",
    "for sp, p in sorted(zip(SPECIES, probs), key=lambda x: -x[1]):\n",
    "    print(f\"{sp:>4}: {p:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# 9) Save best model artifact (optional)\n",
    "# =========================\n",
    "# save_path = f\"/mnt/data/{real.get('target','TARGET').upper()}_{best_name}_best.pt\"\n",
    "# torch.save({\n",
    "#     \"model_type\": best_name,\n",
    "#     \"state_dict\": best_blob[\"state\"],\n",
    "#     \"species\": SPECIES,\n",
    "#     \"n_resample\": N_RESAMPLE,\n",
    "#     \"notes\": \"Trained on synthetic band-mixtures; dry-run model.\"\n",
    "# }, save_path)\n",
    "# print(\"\\nSaved best model to:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10d5a789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.7.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.18.3-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ujwal mojidra\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ujwal mojidra\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\ujwal mojidra\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (2.0.43)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ujwal mojidra\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\ujwal mojidra\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\ujwal mojidra\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\ujwal mojidra\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ujwal mojidra\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\ujwal mojidra\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.7.0-py3-none-any.whl (413 kB)\n",
      "Downloading alembic-1.18.3-py3-none-any.whl (262 kB)\n",
      "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "\n",
      "   ---------------------------------------- 0/4 [Mako]\n",
      "   ---------------------------------------- 0/4 [Mako]\n",
      "   ---------------------------------------- 0/4 [Mako]\n",
      "   ---------- ----------------------------- 1/4 [colorlog]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   -------------------- ------------------- 2/4 [alembic]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ------------------------------ --------- 3/4 [optuna]\n",
      "   ---------------------------------------- 4/4 [optuna]\n",
      "\n",
      "Successfully installed Mako-1.3.10 alembic-1.18.3 colorlog-6.10.1 optuna-4.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b71461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ujwal Mojidra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m[I 2026-02-07 21:36:27,238]\u001b[0m A new study created in memory with name: no-name-cf917426-0dea-430d-9c0f-60c039201daa\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: JUPITER | points: 1024 | device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-07 21:36:30,475]\u001b[0m Trial 0 finished with value: 0.4400597353776296 and parameters: {'h1': 512, 'h2': 128, 'drop1': 0.26468233448057316, 'drop2': 0.1493883376455455, 'lr': 0.00018237626186339215, 'weight_decay': 0.00015924496426865972, 'baseline_win': 101, 'temp': 1.0116894981161977}. Best is trial 0 with value: 0.4400597353776296.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:36:32,797]\u001b[0m Trial 1 finished with value: 0.44394630988438927 and parameters: {'h1': 384, 'h2': 384, 'drop1': 0.28342670912198886, 'drop2': 0.055662307995513416, 'lr': 0.0004943609370698491, 'weight_decay': 5.824647404001128e-06, 'baseline_win': 201, 'temp': 1.5041641564012336}. Best is trial 0 with value: 0.4400597353776296.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:36:34,911]\u001b[0m Trial 2 finished with value: 0.4720622539520264 and parameters: {'h1': 384, 'h2': 128, 'drop1': 0.2342494931461418, 'drop2': 0.17601930928829373, 'lr': 0.0008280591283567678, 'weight_decay': 2.9449016343017413e-05, 'baseline_win': 251, 'temp': 1.0058998567369917}. Best is trial 0 with value: 0.4400597353776296.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:36:37,157]\u001b[0m Trial 3 finished with value: 0.45284026066462196 and parameters: {'h1': 384, 'h2': 384, 'drop1': 0.30529964888145833, 'drop2': 0.06940726801727456, 'lr': 0.0003088500805429374, 'weight_decay': 5.097736733205003e-06, 'baseline_win': 251, 'temp': 1.0735133818078175}. Best is trial 0 with value: 0.4400597353776296.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:36:39,305]\u001b[0m Trial 4 finished with value: 0.4497309386730194 and parameters: {'h1': 384, 'h2': 256, 'drop1': 0.3407349619061868, 'drop2': 0.08152349546314813, 'lr': 0.000449090183408665, 'weight_decay': 5.9622525844674745e-06, 'baseline_win': 201, 'temp': 1.0879039224720428}. Best is trial 0 with value: 0.4400597353776296.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:36:41,064]\u001b[0m Trial 5 finished with value: 0.4373865942160288 and parameters: {'h1': 256, 'h2': 192, 'drop1': 0.14246703056812318, 'drop2': 0.17413591129554923, 'lr': 0.0009390021917713479, 'weight_decay': 1.540805580818781e-06, 'baseline_win': 201, 'temp': 1.0281503219312336}. Best is trial 5 with value: 0.4373865942160288.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:36:43,453]\u001b[0m Trial 6 finished with value: 0.4544412612915039 and parameters: {'h1': 384, 'h2': 128, 'drop1': 0.3319805475936034, 'drop2': 0.2463174712149388, 'lr': 0.000254668339327331, 'weight_decay': 5.6748334399650805e-05, 'baseline_win': 251, 'temp': 1.1723569378791583}. Best is trial 5 with value: 0.4373865942160288.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:36:45,412]\u001b[0m Trial 7 finished with value: 0.4357927223046621 and parameters: {'h1': 384, 'h2': 192, 'drop1': 0.2690025230221327, 'drop2': 0.09508352093975064, 'lr': 0.000499165229590886, 'weight_decay': 1.519222644284282e-06, 'baseline_win': 151, 'temp': 1.646680517977153}. Best is trial 7 with value: 0.4357927223046621.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:36:47,823]\u001b[0m Trial 8 finished with value: 0.4164651075998942 and parameters: {'h1': 384, 'h2': 256, 'drop1': 0.3159570120947055, 'drop2': 0.22453232917738766, 'lr': 0.00021351083300529203, 'weight_decay': 0.00021308532645893647, 'baseline_win': 151, 'temp': 1.6459930406778396}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:36:51,117]\u001b[0m Trial 9 finished with value: 0.44418370525042217 and parameters: {'h1': 768, 'h2': 256, 'drop1': 0.19260290667369373, 'drop2': 0.0075572226806132625, 'lr': 0.00017855291071324576, 'weight_decay': 3.645771432644181e-05, 'baseline_win': 101, 'temp': 1.2075459533037345}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:36:55,254]\u001b[0m Trial 10 finished with value: 0.43734724322954815 and parameters: {'h1': 768, 'h2': 256, 'drop1': 0.1152544709984212, 'drop2': 0.24504085029466283, 'lr': 0.0001081881852893473, 'weight_decay': 0.0002922017734562344, 'baseline_win': 151, 'temp': 1.3969902540582717}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:36:57,813]\u001b[0m Trial 11 finished with value: 0.45533976952234906 and parameters: {'h1': 512, 'h2': 192, 'drop1': 0.23913560390766916, 'drop2': 0.1098352705252701, 'lr': 0.0005249244827738625, 'weight_decay': 1.3256069666835754e-06, 'baseline_win': 151, 'temp': 1.687233196471043}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:00,015]\u001b[0m Trial 12 finished with value: 0.4478838602701823 and parameters: {'h1': 256, 'h2': 192, 'drop1': 0.18732925905710368, 'drop2': 0.20240537577600748, 'lr': 0.00021204448864498357, 'weight_decay': 9.690023413651483e-05, 'baseline_win': 151, 'temp': 1.6914227832896254}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:02,066]\u001b[0m Trial 13 finished with value: 0.46519668896993 and parameters: {'h1': 384, 'h2': 256, 'drop1': 0.2988234321070004, 'drop2': 0.10959420040768397, 'lr': 0.0003461734300621336, 'weight_decay': 1.0867246103121791e-05, 'baseline_win': 151, 'temp': 1.5675025841184882}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:05,060]\u001b[0m Trial 14 finished with value: 0.4555596212546031 and parameters: {'h1': 384, 'h2': 192, 'drop1': 0.26205867501642477, 'drop2': 0.029644234120601742, 'lr': 0.00012350177828180173, 'weight_decay': 2.3358213778225864e-06, 'baseline_win': 151, 'temp': 1.5600304366807718}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:07,425]\u001b[0m Trial 15 finished with value: 0.444087694088618 and parameters: {'h1': 384, 'h2': 192, 'drop1': 0.3143601264819049, 'drop2': 0.13833620667612737, 'lr': 0.0007424911475939635, 'weight_decay': 1.552661251983883e-05, 'baseline_win': 151, 'temp': 1.3925137658812858}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:09,862]\u001b[0m Trial 16 finished with value: 0.42238856554031373 and parameters: {'h1': 512, 'h2': 256, 'drop1': 0.2116778997257759, 'drop2': 0.20725564825568227, 'lr': 0.0006374514845865389, 'weight_decay': 3.0622403519462434e-06, 'baseline_win': 151, 'temp': 1.6108915641037642}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:12,114]\u001b[0m Trial 17 finished with value: 0.4488784154256185 and parameters: {'h1': 512, 'h2': 256, 'drop1': 0.20067631015646123, 'drop2': 0.21268134307328895, 'lr': 0.0006776371941770558, 'weight_decay': 2.944950972141284e-06, 'baseline_win': 151, 'temp': 1.4885966719984274}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:15,102]\u001b[0m Trial 18 finished with value: 0.4429684360822042 and parameters: {'h1': 512, 'h2': 256, 'drop1': 0.17041248442390222, 'drop2': 0.20716487454237392, 'lr': 0.00013983210210230334, 'weight_decay': 0.0002961592110355838, 'baseline_win': 101, 'temp': 1.303120356189867}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:17,436]\u001b[0m Trial 19 finished with value: 0.4236577769120534 and parameters: {'h1': 512, 'h2': 256, 'drop1': 0.21707329221591082, 'drop2': 0.17381188427529823, 'lr': 0.00036605652442090505, 'weight_decay': 1.1272323974457023e-05, 'baseline_win': 151, 'temp': 1.600426201328957}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:20,559]\u001b[0m Trial 20 finished with value: 0.43953822255134584 and parameters: {'h1': 768, 'h2': 256, 'drop1': 0.1546516509438364, 'drop2': 0.2316399985632906, 'lr': 0.0002809311628112876, 'weight_decay': 0.00010810583078713376, 'baseline_win': 151, 'temp': 1.4618118569309821}. Best is trial 8 with value: 0.4164651075998942.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:23,163]\u001b[0m Trial 21 finished with value: 0.41226362784703574 and parameters: {'h1': 512, 'h2': 256, 'drop1': 0.21864223114569922, 'drop2': 0.1792407709503364, 'lr': 0.00037684958354375, 'weight_decay': 1.0520115738007038e-05, 'baseline_win': 151, 'temp': 1.607490761746618}. Best is trial 21 with value: 0.41226362784703574.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:26,085]\u001b[0m Trial 22 finished with value: 0.45153002540270487 and parameters: {'h1': 512, 'h2': 256, 'drop1': 0.21569662199158451, 'drop2': 0.21980049284164593, 'lr': 0.0006422232830061782, 'weight_decay': 3.478725522225167e-06, 'baseline_win': 151, 'temp': 1.620613319494967}. Best is trial 21 with value: 0.41226362784703574.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:28,836]\u001b[0m Trial 23 finished with value: 0.451812607049942 and parameters: {'h1': 512, 'h2': 256, 'drop1': 0.23474820028807677, 'drop2': 0.18767410832298878, 'lr': 0.0004144935977390926, 'weight_decay': 2.5556829129132712e-05, 'baseline_win': 151, 'temp': 1.5426111380332634}. Best is trial 21 with value: 0.41226362784703574.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 21:37:31,584]\u001b[0m Trial 24 finished with value: 0.4548787792523702 and parameters: {'h1': 512, 'h2': 384, 'drop1': 0.2516592673828996, 'drop2': 0.15284431422040973, 'lr': 0.00022646643715829907, 'weight_decay': 7.667770181097744e-06, 'baseline_win': 151, 'temp': 1.6404712285508847}. Best is trial 21 with value: 0.41226362784703574.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best params: {'h1': 512, 'h2': 256, 'drop1': 0.21864223114569922, 'drop2': 0.1792407709503364, 'lr': 0.00037684958354375, 'weight_decay': 1.0520115738007038e-05, 'baseline_win': 151, 'temp': 1.607490761746618}\n",
      "Best objective: 0.41226362784703574\n",
      "Jupiter probs (best trial): {'CH4': 0.8713372945785522, 'NH3': 0.35000383853912354, 'C2H2': 0.2323499470949173, 'C2H6': 0.2261686474084854}\n",
      "\n",
      "FINAL (Optuna-tuned) Jupiter probabilities:\n",
      " CH4: 0.948\n",
      "C2H6: 0.034\n",
      "C2H2: 0.001\n",
      " NH3: 0.000\n",
      "\n",
      "Saved: JUPITER_MLP_OPTUNA.pt\n"
     ]
    }
   ],
   "source": [
    "import pickle, numpy as np, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import optuna\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "PKL_PATH = \"JUPITER_MASTER_SPECTRA.pkl\"\n",
    "\n",
    "SEED = 7\n",
    "N_RESAMPLE = 1024\n",
    "\n",
    "N_SYNTH_FINAL = 4000\n",
    "EPOCHS_FINAL = 25\n",
    "PATIENCE_FINAL = 7\n",
    "BATCH = 128\n",
    "\n",
    "# Optuna tuning budget (keep it small & fast)\n",
    "N_TRIALS = 25\n",
    "N_SYNTH_TUNE = 1600\n",
    "EPOCHS_TUNE = 12\n",
    "PATIENCE_TUNE = 4\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "SPECIES = [\"CH4\", \"NH3\", \"C2H2\", \"C2H6\"]\n",
    "\n",
    "BANDS = {\n",
    "    \"CH4\": [(2350, 260), (2750, 320)],\n",
    "    \"NH3\": [(2050, 140), (2150, 140)],\n",
    "    \"C2H2\": [(2700, 140), (2810, 130)],\n",
    "    \"C2H6\": [(2400, 170), (2550, 150)],\n",
    "}\n",
    "\n",
    "def gaussian(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "# -------------------------\n",
    "# Load + resample\n",
    "# -------------------------\n",
    "with open(PKL_PATH, \"rb\") as f:\n",
    "    real = pickle.load(f)\n",
    "\n",
    "w = np.asarray(real[\"wavelength\"], dtype=float)\n",
    "f = np.asarray(real[\"flux\"], dtype=float)\n",
    "mask = np.isfinite(w) & np.isfinite(f)\n",
    "w, f = w[mask], f[mask]\n",
    "idx = np.argsort(w)\n",
    "w, f = w[idx], f[idx]\n",
    "\n",
    "def resample_to_fixed(wave, flux, n=N_RESAMPLE):\n",
    "    w_new = np.linspace(wave.min(), wave.max(), n)\n",
    "    f_new = np.interp(w_new, wave, flux)\n",
    "    return w_new.astype(np.float32), f_new.astype(np.float32)\n",
    "\n",
    "w_fix, f_fix = resample_to_fixed(w, f, N_RESAMPLE)\n",
    "print(\"Loaded:\", real.get(\"target\",\"unknown\"), \"| points:\", len(w_fix), \"| device:\", device)\n",
    "\n",
    "# -------------------------\n",
    "# Baseline + channels (baseline window is tunable)\n",
    "# -------------------------\n",
    "def compute_baseline(flux, win=151, poly=3):\n",
    "    n = len(flux)\n",
    "    win = int(win)\n",
    "    if win >= n:\n",
    "        win = n-1\n",
    "    if win < 11:\n",
    "        win = 11\n",
    "    if win % 2 == 0:\n",
    "        win += 1\n",
    "    b = savgol_filter(flux.astype(float), window_length=win, polyorder=poly)\n",
    "    eps = 1e-12\n",
    "    b = np.clip(b, np.percentile(b, 1), np.percentile(b, 99)) + eps\n",
    "    return b.astype(np.float32)\n",
    "\n",
    "def make_channels(wave, flux, win):\n",
    "    base = compute_baseline(flux, win=win, poly=3)\n",
    "    r = (flux / base) - 1.0\n",
    "    r = (r - np.median(r)) / (np.std(r) + 1e-8)\n",
    "    d1 = np.gradient(r, wave)\n",
    "    d2 = np.gradient(d1, wave)\n",
    "    return np.stack([r, d1, d2], axis=0).astype(np.float32)\n",
    "\n",
    "# real tensor depends on win, so we compute it inside train functions\n",
    "\n",
    "# -------------------------\n",
    "# Synthetic generator (anchored to REAL baseline)\n",
    "# -------------------------\n",
    "def sample_labels():\n",
    "    y = {sp: 0 for sp in SPECIES}\n",
    "    y[\"CH4\"] = 1 if rng.random() < 0.85 else 0\n",
    "    y[\"NH3\"] = 1 if (y[\"CH4\"] and rng.random() < 0.25) else 0\n",
    "    y[\"C2H2\"] = 1 if rng.random() < 0.20 else 0\n",
    "    y[\"C2H6\"] = 1 if rng.random() < 0.20 else 0\n",
    "    if sum(y.values()) == 0:\n",
    "        y[\"CH4\"] = 1\n",
    "    return y\n",
    "\n",
    "real_baseline_default = compute_baseline(f_fix, win=151)\n",
    "\n",
    "def synth_spectrum(wave, labels, baseline):\n",
    "    cont = baseline.copy()\n",
    "    x = (wave - wave.min()) / (wave.max() - wave.min())\n",
    "    drift = 1.0 + rng.normal(0, 0.01) + rng.normal(0, 0.01) * (x - 0.5)\n",
    "    spec = cont * drift\n",
    "\n",
    "    for sp, present in labels.items():\n",
    "        if not present:\n",
    "            continue\n",
    "        for (c, w0) in BANDS[sp]:\n",
    "            if sp == \"CH4\":\n",
    "                depth = rng.uniform(0.08, 0.28)\n",
    "            elif sp == \"C2H2\":\n",
    "                depth = rng.uniform(0.02, 0.10)\n",
    "            else:\n",
    "                depth = rng.uniform(0.03, 0.18)\n",
    "\n",
    "            width = w0 * rng.uniform(0.85, 1.25)\n",
    "            c_jit = c + rng.normal(0, 15)\n",
    "            dip = 1.0 - depth * gaussian(wave, c_jit, width)\n",
    "            spec *= dip\n",
    "\n",
    "    sigma = 0.01 * (np.max(spec) - np.min(spec) + 1e-8)\n",
    "    noise = rng.normal(0, sigma, size=wave.shape[0])\n",
    "    noise = np.convolve(noise, np.ones(7)/7, mode=\"same\")\n",
    "    return (spec + noise).astype(np.float32)\n",
    "\n",
    "def build_dataset(wave, n, win):\n",
    "    # baseline used for synthesis: compute once per run\n",
    "    baseline = compute_baseline(f_fix, win=win, poly=3)\n",
    "    X_list, Y_list = [], []\n",
    "    for _ in range(n):\n",
    "        lab = sample_labels()\n",
    "        spec = synth_spectrum(wave, lab, baseline)\n",
    "        X = make_channels(wave, spec, win=win)\n",
    "        y = np.array([lab[sp] for sp in SPECIES], dtype=np.float32)\n",
    "        X_list.append(X)\n",
    "        Y_list.append(y)\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    Y = np.stack(Y_list, axis=0)\n",
    "    perm = rng.permutation(len(X))\n",
    "    X, Y = X[perm], Y[perm]\n",
    "    n_train = int(0.85 * len(X))\n",
    "    return X[:n_train], Y[:n_train], X[n_train:], Y[n_train:]\n",
    "\n",
    "# -------------------------\n",
    "# Train function (used by Optuna and final training)\n",
    "# -------------------------\n",
    "def train_mlp_once(h1, h2, drop1, drop2, lr, wd, win, n_synth, epochs, patience):\n",
    "    X_train, Y_train, X_val, Y_val = build_dataset(w_fix, n=n_synth, win=win)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(X_train), torch.tensor(Y_train)),\n",
    "        batch_size=BATCH, shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(X_val), torch.tensor(Y_val)),\n",
    "        batch_size=BATCH, shuffle=False\n",
    "    )\n",
    "\n",
    "    C, N = 3, N_RESAMPLE\n",
    "    K = len(SPECIES)\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(C*N, h1),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop1),\n",
    "                nn.Linear(h1, h2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop2),\n",
    "                nn.Linear(h2, K)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    model = MLP().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_val():\n",
    "        model.eval()\n",
    "        tot = 0.0\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            tot += loss_fn(model(xb), yb).item() * xb.size(0)\n",
    "        return tot / len(X_val)\n",
    "\n",
    "    best_loss, best_state, bad = 1e9, None, 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "        vloss = eval_val()\n",
    "        if vloss < best_loss - 1e-4:\n",
    "            best_loss = vloss\n",
    "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    # reload best\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "\n",
    "    # real Jupiter inference (for sanity / to store)\n",
    "    X_real = make_channels(w_fix, f_fix, win=win)\n",
    "    X_real_t = torch.tensor(X_real[None, ...], dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_real_t)[0].cpu().numpy()\n",
    "\n",
    "    return best_loss, logits, best_state\n",
    "\n",
    "# -------------------------\n",
    "# Optuna objective\n",
    "# -------------------------\n",
    "def objective(trial):\n",
    "    h1 = trial.suggest_categorical(\"h1\", [256, 384, 512, 768])\n",
    "    h2 = trial.suggest_categorical(\"h2\", [128, 192, 256, 384])\n",
    "    drop1 = trial.suggest_float(\"drop1\", 0.10, 0.35)\n",
    "    drop2 = trial.suggest_float(\"drop2\", 0.00, 0.25)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True)\n",
    "    wd = trial.suggest_float(\"weight_decay\", 1e-6, 3e-4, log=True)\n",
    "    win = trial.suggest_categorical(\"baseline_win\", [101, 151, 201, 251])\n",
    "    T = trial.suggest_float(\"temp\", 1.0, 1.7)\n",
    "\n",
    "    vloss, logits, _ = train_mlp_once(\n",
    "        h1=h1, h2=h2, drop1=drop1, drop2=drop2,\n",
    "        lr=lr, wd=wd, win=win,\n",
    "        n_synth=N_SYNTH_TUNE,\n",
    "        epochs=EPOCHS_TUNE,\n",
    "        patience=PATIENCE_TUNE\n",
    "    )\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits / T))\n",
    "    ch4 = float(probs[SPECIES.index(\"CH4\")])\n",
    "\n",
    "    # light sanity penalty to avoid \"CH4 near zero\" configs\n",
    "    penalty = 0.0\n",
    "    if ch4 < 0.5:\n",
    "        penalty += (0.5 - ch4) * 2.0\n",
    "\n",
    "    trial.set_user_attr(\"jupiter_probs\", {sp: float(p) for sp, p in zip(SPECIES, probs)})\n",
    "    return float(vloss + penalty)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "print(\"\\nBest params:\", study.best_params)\n",
    "print(\"Best objective:\", study.best_value)\n",
    "print(\"Jupiter probs (best trial):\", study.best_trial.user_attrs[\"jupiter_probs\"])\n",
    "\n",
    "# -------------------------\n",
    "# Final train with best params on full synth\n",
    "# -------------------------\n",
    "bp = study.best_params\n",
    "final_vloss, final_logits, final_state = train_mlp_once(\n",
    "    h1=bp[\"h1\"], h2=bp[\"h2\"], drop1=bp[\"drop1\"], drop2=bp[\"drop2\"],\n",
    "    lr=bp[\"lr\"], wd=bp[\"weight_decay\"], win=bp[\"baseline_win\"],\n",
    "    n_synth=N_SYNTH_FINAL, epochs=EPOCHS_FINAL, patience=PATIENCE_FINAL\n",
    ")\n",
    "\n",
    "T = bp[\"temp\"]\n",
    "final_probs = 1 / (1 + np.exp(-final_logits / T))\n",
    "\n",
    "print(\"\\nFINAL (Optuna-tuned) Jupiter probabilities:\")\n",
    "for sp, p in sorted(zip(SPECIES, final_probs), key=lambda x: -x[1]):\n",
    "    print(f\"{sp:>4}: {p:.3f}\")\n",
    "\n",
    "# Optional: save tuned model\n",
    "save_path = \"JUPITER_MLP_OPTUNA.pt\"\n",
    "torch.save({\n",
    "    \"planet\": \"JUPITER\",\n",
    "    \"species\": SPECIES,\n",
    "    \"bands\": BANDS,\n",
    "    \"best_params\": bp,\n",
    "    \"val_loss\": final_vloss,\n",
    "    \"state_dict\": final_state,\n",
    "    \"n_resample\": N_RESAMPLE,\n",
    "}, save_path)\n",
    "print(\"\\nSaved:\", save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
