{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b71461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-08 17:46:11,109]\u001b[0m A new study created in memory with name: no-name-b6e0d86c-3723-477f-80b6-89c905785425\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: JUPITER | points: 1024 | device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-08 17:46:15,332]\u001b[0m Trial 0 finished with value: 0.45023446083068847 and parameters: {'h1': 256, 'h2': 256, 'drop1': 0.25237939807920184, 'drop2': 0.04207927375960574, 'lr': 0.0001998040602020569, 'weight_decay': 5.4752328836882275e-06, 'baseline_win': 101, 'temp': 1.2371647590388182}. Best is trial 0 with value: 0.45023446083068847.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:18,008]\u001b[0m Trial 1 finished with value: 0.41696301897366844 and parameters: {'h1': 384, 'h2': 192, 'drop1': 0.19203631002213528, 'drop2': 0.09483857646754132, 'lr': 0.00013131511314116314, 'weight_decay': 2.1306818578067545e-05, 'baseline_win': 101, 'temp': 1.5271151217442496}. Best is trial 1 with value: 0.41696301897366844.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:20,778]\u001b[0m Trial 2 finished with value: 0.4391616145769755 and parameters: {'h1': 512, 'h2': 256, 'drop1': 0.21658611187466575, 'drop2': 0.10816339259499558, 'lr': 0.0003118040949231639, 'weight_decay': 2.3166643234161838e-06, 'baseline_win': 151, 'temp': 1.1548509225131474}. Best is trial 1 with value: 0.41696301897366844.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:23,421]\u001b[0m Trial 3 finished with value: 0.4511910080909729 and parameters: {'h1': 384, 'h2': 256, 'drop1': 0.24203410582971935, 'drop2': 0.08178027652689976, 'lr': 0.0001570313997439996, 'weight_decay': 9.10622447558115e-05, 'baseline_win': 251, 'temp': 1.122114319883972}. Best is trial 1 with value: 0.41696301897366844.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:25,958]\u001b[0m Trial 4 finished with value: 0.443297795454661 and parameters: {'h1': 256, 'h2': 384, 'drop1': 0.2159298887315519, 'drop2': 0.08553260274159913, 'lr': 0.00011631762981373704, 'weight_decay': 5.258011018588034e-05, 'baseline_win': 251, 'temp': 1.416081135805813}. Best is trial 1 with value: 0.41696301897366844.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:28,601]\u001b[0m Trial 5 finished with value: 0.40910313924153646 and parameters: {'h1': 768, 'h2': 384, 'drop1': 0.336895248086769, 'drop2': 0.16345317973955314, 'lr': 0.0005696606733927714, 'weight_decay': 4.925383981588028e-06, 'baseline_win': 251, 'temp': 1.459889808736237}. Best is trial 5 with value: 0.40910313924153646.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:31,280]\u001b[0m Trial 6 finished with value: 0.44335113565127054 and parameters: {'h1': 768, 'h2': 384, 'drop1': 0.1532603688940839, 'drop2': 0.0347907737773282, 'lr': 0.00037857466086922904, 'weight_decay': 1.9584211708547446e-05, 'baseline_win': 251, 'temp': 1.4164226409250464}. Best is trial 5 with value: 0.40910313924153646.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:33,201]\u001b[0m Trial 7 finished with value: 0.43570003906885785 and parameters: {'h1': 256, 'h2': 384, 'drop1': 0.13714064963147235, 'drop2': 0.08079326736574946, 'lr': 0.0003056953885234741, 'weight_decay': 0.00018980020658385116, 'baseline_win': 151, 'temp': 1.5223974958503326}. Best is trial 5 with value: 0.40910313924153646.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:35,648]\u001b[0m Trial 8 finished with value: 0.43102239569028217 and parameters: {'h1': 256, 'h2': 256, 'drop1': 0.21841961233055807, 'drop2': 0.134831399320239, 'lr': 0.00011271567956837479, 'weight_decay': 2.4289948111480334e-05, 'baseline_win': 201, 'temp': 1.0612307057442016}. Best is trial 5 with value: 0.40910313924153646.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:38,732]\u001b[0m Trial 9 finished with value: 0.43771194815635683 and parameters: {'h1': 768, 'h2': 256, 'drop1': 0.1661987367266461, 'drop2': 0.17985578501827057, 'lr': 0.00019283475572660498, 'weight_decay': 1.2877598633126587e-05, 'baseline_win': 251, 'temp': 1.5629473783120118}. Best is trial 5 with value: 0.40910313924153646.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:41,109]\u001b[0m Trial 10 finished with value: 0.4565425217151642 and parameters: {'h1': 768, 'h2': 128, 'drop1': 0.3429654102799541, 'drop2': 0.24347097135493448, 'lr': 0.0009398021901069359, 'weight_decay': 1.101318721278346e-06, 'baseline_win': 201, 'temp': 1.6530358426710043}. Best is trial 5 with value: 0.40910313924153646.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:43,029]\u001b[0m Trial 11 finished with value: 0.3833982288837433 and parameters: {'h1': 384, 'h2': 192, 'drop1': 0.33524365842588316, 'drop2': 0.1651058652976325, 'lr': 0.0007110310543639465, 'weight_decay': 4.907620008540501e-06, 'baseline_win': 101, 'temp': 1.3345064815359007}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:44,943]\u001b[0m Trial 12 finished with value: 0.42632426222165426 and parameters: {'h1': 384, 'h2': 192, 'drop1': 0.34527803560901976, 'drop2': 0.17581259914122, 'lr': 0.0007635160133846796, 'weight_decay': 4.791010209798084e-06, 'baseline_win': 101, 'temp': 1.2865277725738666}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:47,227]\u001b[0m Trial 13 finished with value: 0.4694768443703652 and parameters: {'h1': 512, 'h2': 192, 'drop1': 0.2921105382827546, 'drop2': 0.17196692177445044, 'lr': 0.0005659612119722578, 'weight_decay': 6.138733502905724e-06, 'baseline_win': 101, 'temp': 1.3645057095315671}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:49,546]\u001b[0m Trial 14 finished with value: 0.4644192039966583 and parameters: {'h1': 384, 'h2': 128, 'drop1': 0.30300059548058655, 'drop2': 0.2226471306686578, 'lr': 0.0005833652285112775, 'weight_decay': 2.0993829840761075e-06, 'baseline_win': 251, 'temp': 1.2715908083205232}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:51,929]\u001b[0m Trial 15 finished with value: 0.45276619990666706 and parameters: {'h1': 768, 'h2': 192, 'drop1': 0.10571034487192732, 'drop2': 0.14305901412259106, 'lr': 0.00048344839450500514, 'weight_decay': 2.8873534126146593e-06, 'baseline_win': 101, 'temp': 1.4516340357651614}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:53,961]\u001b[0m Trial 16 finished with value: 0.4557929396629333 and parameters: {'h1': 384, 'h2': 384, 'drop1': 0.2968562645969934, 'drop2': 0.20383759460118694, 'lr': 0.000762693449798994, 'weight_decay': 1.0168577746729826e-05, 'baseline_win': 201, 'temp': 1.6641374492084027}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:56,463]\u001b[0m Trial 17 finished with value: 0.4158926943937937 and parameters: {'h1': 768, 'h2': 384, 'drop1': 0.3183214992039448, 'drop2': 0.15337358063160805, 'lr': 0.0004265663201895649, 'weight_decay': 8.544952332358155e-06, 'baseline_win': 151, 'temp': 1.3442651047873286}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:46:58,392]\u001b[0m Trial 18 finished with value: 0.4369157612323761 and parameters: {'h1': 512, 'h2': 192, 'drop1': 0.2649676854687304, 'drop2': 0.20760015460694214, 'lr': 0.0007081509511758608, 'weight_decay': 1.1191852409531287e-06, 'baseline_win': 101, 'temp': 1.2023207556510822}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:47:01,482]\u001b[0m Trial 19 finished with value: 0.42285474538803103 and parameters: {'h1': 768, 'h2': 128, 'drop1': 0.3243788016295376, 'drop2': 0.12350344600032362, 'lr': 0.0002374812789302756, 'weight_decay': 3.943142994588366e-05, 'baseline_win': 251, 'temp': 1.5873739919416496}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:47:03,653]\u001b[0m Trial 20 finished with value: 0.4387907187143962 and parameters: {'h1': 384, 'h2': 384, 'drop1': 0.2805216849154798, 'drop2': 0.1588159777798343, 'lr': 0.000985988043078458, 'weight_decay': 3.256878044002736e-06, 'baseline_win': 251, 'temp': 1.470975644512519}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:47:06,184]\u001b[0m Trial 21 finished with value: 0.44807610909144086 and parameters: {'h1': 768, 'h2': 384, 'drop1': 0.3205343043180867, 'drop2': 0.15702580262563648, 'lr': 0.0004261637244101142, 'weight_decay': 8.619836548290897e-06, 'baseline_win': 151, 'temp': 1.331370591345445}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:47:09,009]\u001b[0m Trial 22 finished with value: 0.4187769095102946 and parameters: {'h1': 768, 'h2': 384, 'drop1': 0.3196547903077222, 'drop2': 0.19462692141138097, 'lr': 0.0005820283727182209, 'weight_decay': 7.632368034437943e-06, 'baseline_win': 151, 'temp': 1.3458046965634074}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:47:11,504]\u001b[0m Trial 23 finished with value: 0.4530469238758087 and parameters: {'h1': 768, 'h2': 384, 'drop1': 0.34948981477775665, 'drop2': 0.11900691943874811, 'lr': 0.00039084648665615947, 'weight_decay': 4.195047883848181e-06, 'baseline_win': 151, 'temp': 1.3708740352205777}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n",
      "\u001b[32m[I 2026-02-08 17:47:13,916]\u001b[0m Trial 24 finished with value: 0.44518377582232155 and parameters: {'h1': 768, 'h2': 192, 'drop1': 0.3245402553920837, 'drop2': 0.1513769615764936, 'lr': 0.0004928008818753372, 'weight_decay': 1.6564047110707713e-06, 'baseline_win': 151, 'temp': 1.2897460666894585}. Best is trial 11 with value: 0.3833982288837433.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best params: {'h1': 384, 'h2': 192, 'drop1': 0.33524365842588316, 'drop2': 0.1651058652976325, 'lr': 0.0007110310543639465, 'weight_decay': 4.907620008540501e-06, 'baseline_win': 101, 'temp': 1.3345064815359007}\n",
      "Best objective: 0.3833982288837433\n",
      "Jupiter probs (best trial): {'CH4': 0.9137865304946899, 'NH3': 0.1581314653158188, 'C2H2': 0.48879021406173706, 'C2H6': 0.5005111694335938}\n",
      "\n",
      "FINAL (Optuna-tuned) Jupiter probabilities:\n",
      " CH4: 0.974\n",
      "C2H6: 0.073\n",
      "C2H2: 0.052\n",
      " NH3: 0.007\n",
      "\n",
      "Saved: JUPITER_MLP_OPTUNA.pt\n"
     ]
    }
   ],
   "source": [
    "import pickle, numpy as np, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.signal import savgol_filter\n",
    "import optuna\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "PKL_PATH = r\"Spectral Analysis Planets pkl\\JUPITER_MASTER_SPECTRA.pkl\"\n",
    "\n",
    "SEED = 7\n",
    "N_RESAMPLE = 1024\n",
    "\n",
    "# Final training\n",
    "N_SYNTH_FINAL = 4000\n",
    "EPOCHS_FINAL = 25\n",
    "PATIENCE_FINAL = 7\n",
    "\n",
    "BATCH = 128\n",
    "\n",
    "# Optuna\n",
    "N_TRIALS = 25\n",
    "N_SYNTH_TUNE = 1600\n",
    "EPOCHS_TUNE = 12\n",
    "PATIENCE_TUNE = 4\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "SPECIES = [\"CH4\", \"NH3\", \"C2H2\", \"C2H6\"]\n",
    "\n",
    "BANDS = {\n",
    "    \"CH4\": [(2350, 260), (2750, 320)],\n",
    "    \"NH3\": [(2050, 140), (2150, 140)],\n",
    "    \"C2H2\": [(2700, 140), (2810, 130)],\n",
    "    \"C2H6\": [(2400, 170), (2550, 150)],\n",
    "}\n",
    "\n",
    "def gaussian(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "# -------------------------\n",
    "# Load + resample\n",
    "# -------------------------\n",
    "with open(PKL_PATH, \"rb\") as f:\n",
    "    real = pickle.load(f)\n",
    "\n",
    "w = np.asarray(real[\"wavelength\"], dtype=float)\n",
    "f = np.asarray(real[\"flux\"], dtype=float)\n",
    "mask = np.isfinite(w) & np.isfinite(f)\n",
    "w, f = w[mask], f[mask]\n",
    "idx = np.argsort(w)\n",
    "w, f = w[idx], f[idx]\n",
    "\n",
    "def resample_to_fixed(wave, flux, n=N_RESAMPLE):\n",
    "    w_new = np.linspace(wave.min(), wave.max(), n)\n",
    "    f_new = np.interp(w_new, wave, flux)\n",
    "    return w_new.astype(np.float32), f_new.astype(np.float32)\n",
    "\n",
    "w_fix, f_fix = resample_to_fixed(w, f, N_RESAMPLE)\n",
    "print(\"Loaded:\", real.get(\"target\",\"unknown\"), \"| points:\", len(w_fix), \"| device:\", device)\n",
    "\n",
    "# -------------------------\n",
    "# Baseline + channels (baseline window is tunable)\n",
    "# -------------------------\n",
    "def compute_baseline(flux, win=151, poly=3):\n",
    "    n = len(flux)\n",
    "    win = int(win)\n",
    "    if win >= n:\n",
    "        win = n - 1\n",
    "    if win < 11:\n",
    "        win = 11\n",
    "    if win % 2 == 0:\n",
    "        win += 1\n",
    "    b = savgol_filter(flux.astype(float), window_length=win, polyorder=poly)\n",
    "    eps = 1e-12\n",
    "    b = np.clip(b, np.percentile(b, 1), np.percentile(b, 99)) + eps\n",
    "    return b.astype(np.float32)\n",
    "\n",
    "def make_channels(wave, flux, win):\n",
    "    base = compute_baseline(flux, win=win, poly=3)\n",
    "    r = (flux / base) - 1.0\n",
    "    r = (r - np.median(r)) / (np.std(r) + 1e-8)\n",
    "    d1 = np.gradient(r, wave)\n",
    "    d2 = np.gradient(d1, wave)\n",
    "    return np.stack([r, d1, d2], axis=0).astype(np.float32)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset builder (DETERMINISTIC via seed)\n",
    "# -------------------------\n",
    "def build_dataset(wave, n, win, seed):\n",
    "    local_rng = np.random.default_rng(seed)\n",
    "    baseline = compute_baseline(f_fix, win=win, poly=3)\n",
    "\n",
    "    def sample_labels_local():\n",
    "        y = {sp: 0 for sp in SPECIES}\n",
    "        y[\"CH4\"] = 1 if local_rng.random() < 0.85 else 0\n",
    "        y[\"NH3\"] = 1 if (y[\"CH4\"] and local_rng.random() < 0.25) else 0\n",
    "        y[\"C2H2\"] = 1 if local_rng.random() < 0.20 else 0\n",
    "        y[\"C2H6\"] = 1 if local_rng.random() < 0.20 else 0\n",
    "        if sum(y.values()) == 0:\n",
    "            y[\"CH4\"] = 1\n",
    "        return y\n",
    "\n",
    "    def synth_spectrum_local(labels):\n",
    "        cont = baseline.copy()\n",
    "        x = (wave - wave.min()) / (wave.max() - wave.min())\n",
    "        drift = 1.0 + local_rng.normal(0, 0.01) + local_rng.normal(0, 0.01) * (x - 0.5)\n",
    "        spec = cont * drift\n",
    "\n",
    "        for sp, present in labels.items():\n",
    "            if not present:\n",
    "                continue\n",
    "            for (c, w0) in BANDS[sp]:\n",
    "                if sp == \"CH4\":\n",
    "                    depth = local_rng.uniform(0.08, 0.28)\n",
    "                elif sp == \"C2H2\":\n",
    "                    depth = local_rng.uniform(0.02, 0.10)\n",
    "                else:\n",
    "                    depth = local_rng.uniform(0.03, 0.18)\n",
    "\n",
    "                width = w0 * local_rng.uniform(0.85, 1.25)\n",
    "                c_jit = c + local_rng.normal(0, 15)\n",
    "                spec *= (1.0 - depth * gaussian(wave, c_jit, width))\n",
    "\n",
    "        sigma = 0.01 * (np.max(spec) - np.min(spec) + 1e-8)\n",
    "        noise = local_rng.normal(0, sigma, size=wave.shape[0])\n",
    "        noise = np.convolve(noise, np.ones(7)/7, mode=\"same\")\n",
    "        return (spec + noise).astype(np.float32)\n",
    "\n",
    "    X_list, Y_list = [], []\n",
    "    for _ in range(n):\n",
    "        lab = sample_labels_local()\n",
    "        spec = synth_spectrum_local(lab)\n",
    "        X_list.append(make_channels(wave, spec, win=win))\n",
    "        Y_list.append(np.array([lab[sp] for sp in SPECIES], dtype=np.float32))\n",
    "\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    Y = np.stack(Y_list, axis=0)\n",
    "\n",
    "    perm = local_rng.permutation(len(X))\n",
    "    X, Y = X[perm], Y[perm]\n",
    "    n_train = int(0.85 * len(X))\n",
    "    return X[:n_train], Y[:n_train], X[n_train:], Y[n_train:]\n",
    "\n",
    "# -------------------------\n",
    "# Train MLP once\n",
    "# -------------------------\n",
    "def train_mlp_once(h1, h2, drop1, drop2, lr, wd, win, n_synth, epochs, patience, seed):\n",
    "    X_train, Y_train, X_val, Y_val = build_dataset(w_fix, n=n_synth, win=win, seed=seed)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(X_train), torch.tensor(Y_train)),\n",
    "        batch_size=BATCH, shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(X_val), torch.tensor(Y_val)),\n",
    "        batch_size=BATCH, shuffle=False\n",
    "    )\n",
    "\n",
    "    C, N = 3, N_RESAMPLE\n",
    "    K = len(SPECIES)\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(C*N, h1),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop1),\n",
    "                nn.Linear(h1, h2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop2),\n",
    "                nn.Linear(h2, K)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    model = MLP().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_val():\n",
    "        model.eval()\n",
    "        tot = 0.0\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            tot += loss_fn(model(xb), yb).item() * xb.size(0)\n",
    "        return tot / len(X_val)\n",
    "\n",
    "    best_loss, best_state, bad = 1e9, None, 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "        vloss = eval_val()\n",
    "        if vloss < best_loss - 1e-4:\n",
    "            best_loss = vloss\n",
    "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "\n",
    "    # Real Jupiter inference logits (for Optuna + final report)\n",
    "    X_real = make_channels(w_fix, f_fix, win=win)\n",
    "    X_real_t = torch.tensor(X_real[None, ...], dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        real_logits = model(X_real_t)[0].cpu().numpy()\n",
    "\n",
    "    return float(best_loss), real_logits, best_state\n",
    "\n",
    "# -------------------------\n",
    "# Optuna objective\n",
    "# -------------------------\n",
    "def objective(trial):\n",
    "    # suggest first\n",
    "    h1 = trial.suggest_categorical(\"h1\", [256, 384, 512, 768])\n",
    "    h2 = trial.suggest_categorical(\"h2\", [128, 192, 256, 384])\n",
    "    drop1 = trial.suggest_float(\"drop1\", 0.10, 0.35)\n",
    "    drop2 = trial.suggest_float(\"drop2\", 0.00, 0.25)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True)\n",
    "    wd = trial.suggest_float(\"weight_decay\", 1e-6, 3e-4, log=True)\n",
    "    win = trial.suggest_categorical(\"baseline_win\", [101, 151, 201, 251])\n",
    "    T = trial.suggest_float(\"temp\", 1.0, 1.7)\n",
    "\n",
    "    seed = 10_000 + trial.number\n",
    "\n",
    "    vloss, logits, _ = train_mlp_once(\n",
    "        h1=h1, h2=h2, drop1=drop1, drop2=drop2,\n",
    "        lr=lr, wd=wd, win=win,\n",
    "        n_synth=N_SYNTH_TUNE,\n",
    "        epochs=EPOCHS_TUNE,\n",
    "        patience=PATIENCE_TUNE,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits / T))\n",
    "\n",
    "    penalty = 0.0\n",
    "    ch4 = float(probs[SPECIES.index(\"CH4\")])\n",
    "    if ch4 < 0.5:\n",
    "        penalty += (0.5 - ch4) * 2.0\n",
    "\n",
    "    non_ch4_max = float(np.max([probs[SPECIES.index(s)] for s in SPECIES if s != \"CH4\"]))\n",
    "    if non_ch4_max < 0.10:\n",
    "        penalty += (0.10 - non_ch4_max) * 1.0\n",
    "\n",
    "    trial.set_user_attr(\"jupiter_probs\", {sp: float(p) for sp, p in zip(SPECIES, probs)})\n",
    "    return float(vloss + penalty)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "print(\"\\nBest params:\", study.best_params)\n",
    "print(\"Best objective:\", study.best_value)\n",
    "print(\"Jupiter probs (best trial):\", study.best_trial.user_attrs[\"jupiter_probs\"])\n",
    "\n",
    "# -------------------------\n",
    "# Final train with best params\n",
    "# -------------------------\n",
    "bp = study.best_params\n",
    "FINAL_SEED = 999\n",
    "\n",
    "final_vloss, final_logits, final_state = train_mlp_once(\n",
    "    h1=bp[\"h1\"], h2=bp[\"h2\"], drop1=bp[\"drop1\"], drop2=bp[\"drop2\"],\n",
    "    lr=bp[\"lr\"], wd=bp[\"weight_decay\"], win=bp[\"baseline_win\"],\n",
    "    n_synth=N_SYNTH_FINAL, epochs=EPOCHS_FINAL, patience=PATIENCE_FINAL,\n",
    "    seed=FINAL_SEED\n",
    ")\n",
    "\n",
    "T = bp[\"temp\"]\n",
    "final_probs = 1 / (1 + np.exp(-final_logits / T))\n",
    "\n",
    "print(\"\\nFINAL (Optuna-tuned) Jupiter probabilities:\")\n",
    "for sp, p in sorted(zip(SPECIES, final_probs), key=lambda x: -x[1]):\n",
    "    print(f\"{sp:>4}: {p:.3f}\")\n",
    "\n",
    "# Save tuned model\n",
    "save_path = \"JUPITER_MLP_OPTUNA.pt\"\n",
    "torch.save({\n",
    "    \"planet\": \"JUPITER\",\n",
    "    \"species\": SPECIES,\n",
    "    \"bands\": BANDS,\n",
    "    \"best_params\": bp,\n",
    "    \"temp\": T,\n",
    "    \"val_loss\": final_vloss,\n",
    "    \"state_dict\": final_state,\n",
    "    \"n_resample\": N_RESAMPLE,\n",
    "}, save_path)\n",
    "print(\"\\nSaved:\", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
